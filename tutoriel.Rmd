---
title: "Atelier Statistiques Avancées"
subtitle: "28e Colloque du Chapitre Saint-Laurent"
author:
  name: "Maxime Fraser Franco"
  affiliation: |
    Département des Sciences Biologiques \n Université du Québec à Montréal (UQAM)
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output:
  rmdformats::readthedown:
    df_print: paged
    code_folding: show
bibliography: refs.bib
nocite: |
  @McElreath2020
  @Burkner2017
  @Burkner2018
  @Piironen.Vehtari2017
  @aczelDiscussionPointsBayesian2020
  @kruschkeBayesianAnalysisReporting2021
  @Vehtari.etal2017
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.align = "center",
  warning = FALSE,
  message = FALSE
)

library(data.table)
library(brms)
library(bayesplot)
library(ggplot2)
library(ggpubr)
library(viridis)
library(knitr)
```

# 1. Introduction

## Contexte

Ce tutoriel a été développé dans le cadre du 28e colloque du Chapitre Saint-Laurent, présenté le 29 Mai 2024 à l'Université Laval, Québec, Canada.

Nous verrons les concepts de base qui vous permettront d'implémenter des modèles hiérarchiques avec le logiciel R, et d'interpréter les résultats en utilisant l'inférence Bayésienne.

Les objectifs d'apprentissage sont:  

1. De se familiariser avec la théorie sur les modèles linéaires hiérarchiques
2. De reconnaître les variantes des modèles linéaires ((H)(G)LM)
3. D'implémenter un modèle hiéarchique avec R
4. D'interpréter les résultats avec l'inférence Bayésienne

## librairies

Voici la liste des librairies que nous allons utiliser pour l'atelier. Assurez-vous de les avoir téléchargées.

```{r, eval=FALSE}
# Pour manipuler les données
library(data.table)

# Pour estimer les paramètres des modèles
library(brms)

# Pour diagnostiquer les modèles bayésiens
library(bayesplot)

# Pour nos graphiques
library(ggplot2)
library(ggpubr)
library(viridis)
library(ggExtra)

# Pour générer de jolies tables
library(knitr)
```

## Jeu de données

Le jeu de données que nous utiliserons pour l'atelier se nomme ADORE et provient de l'article suivant:

[Schür, C., Gasser, L., Perez-Cruz, F., Schirmer, K., Baity-Jesi, M. (2023). A benchmark dataset for machine learning in ecotoxicology. *Scientific Data*, 10:718. https://doi.org/10.1038/s41597-023-02612-2](https://www.nature.com/articles/s41597-023-02612-2)

Vous pouvez trouver les données originales sur le portail du [Eawag Research Data Institutional Collection](https://opendata.eawag.ch/dataset/adore).

Le jeu de données ADORE contient une myriade d'informations sur des expériences écotoxicologiques effectuées sur des espèces de poisson, de crustacés, et d'algues. L'objectif des données ADORE est de concerter des informations sur la toxicité aquatique accrue de différents composés chimiques sur des espèces pour faire de la modélisation prédictive.

## Modèles

Les modèles estimés durant l'atelier peuvent prendre un certain temps. Après avoir téléchargé les dossiers de l'atelier (dans ce dépôt GitHub)[https://github.com/quantitative-ecologist/Atelier-CSL-EcotoQ], vous pouvez effectuer ces commandes pour les importer dans votre session:

```{r , include=FALSE, eval=FALSE}
glm1 <- readRDS(
  file = file.path(getwd(), "outputs", "glm1.rds")
)

hglm <- readRDS(
  file = file.path(getwd(), "outputs", "hglm.rds")
)
```

Cela vous évitera d'avoir à estimer les paramètres durant l'atelier, mais vous pourrez tout autant manipuler les résultats et produire les figures.

# 2. Une note sur l'inférence Bayésienne

L'inférence Bayésienne est une toute autre façon de penser pour la réalisation et l'interprétation d'analyses statistiques. Cela vous apparaîtra comme assez différent de ce à quoi vous êtes habitué.e.s si vous avez appris les statistiques classiques (fréquentistes).

## Le théorème de Bayes

En inférence Bayésienne, on se sert de nos connaissances à priori ainsi que des données pour faire de l'inférence. Nous pouvons représenter ceci avec le théorème de Bayes qui va comme suit:

$$P(H \mid E) = \frac{P(H) \ P(E \mid H)}{P(E)}$$

Pour résumer l'équation:  

- $P(H)$ est la probabilité d'observer notre hypothèse, c'est la distribution à priori
- $P(E)$ est la probabilité d'observer de nouvelles preuves sous toutes les hypothèses possibles
- $P(H \mid E)$ est la probabilité d'observer notre hypothèse selon l'évidence, c'est la distribution à postériori
- $P(E \mid H)$ est la probabilité d'observer l'évidence si notre hypothèse est vraie, c'est la distribution de vraisemblance


Plus intuitivement, on peut aussi l'illustrer comme suit:

```{r, echo = FALSE, out.width = "100%", fig.align = "center", fig.cap = "Illustration du théorème de Bayes issue de Yanagisawa et al. 2019"}
knitr::include_graphics("images/bayes-inf.png")
```

**Ainsi, l'inférence Bayésienne est une approche où l'on adapte nos connaissances (à posteriori) à partir de nos hypothèses (connaissances à priori) et les preuves que nous avons.**

La distribution à posteriori est donc l'élément central que nous cherchons à estimer. Rappelez-vous que chaque fois que vous voyez le terme "à posteriori" dans ce tutoriel, nous faisons référence à la distribution d'un paramètre d'intérêt estimé par le modèle.

En prime, voici un lien vers une application qui vous permet de jouer avec ces trois distributions pour mieux comprendre comment les connaissances à priori et les preuves modifient la distribution à posteriori: 

https://micl.shinyapps.io/prior2post/

## Méthode d'estimation

Il existe de nombreuses façons d'approximer la distribution à posteriori. Dans ce tutoriel, nous utiliserons la méthode de Monte Carlo Hamiltonienne (HMC) et l'échantillonneur No-U-Turn (NUTS) pour estimer la distribution à posteriori. Ce sont des algorithmes spécifiques dans la grande famille d'estimation de Monte Carlo par chaîne de Markov (MCMC). 

Pour ce faire nous effectuerons tous nos modèles avec la librairie `brms`, une interface R pour le [langage probabiliste STAN](https://mc-stan.org/). La librairie `brms` utilise l'algorithme NUTS pour estimer les paramètres. Je vous recommande vivement de lire [les vignettes de la librairie](https://paul-buerkner.github.io/brms/articles/index.html) pour mieux comprendre les capacités et le fonctionnement de `brms`.

## Les avantages

L'inférence Bayésienne comporte certains avantages par rapport à l'approche statistique fréquentiste.

- Elle met l'accent sur la distribution et l'incertitude au lieu d'estimés uniques
- Elle permet une interprétation plus naturelle (probabiliste)
- On peut facilement critiquer les modèles

## Pour aller plus loin

La théorie sous-jacente au théorème de Bayes dépasse largement le cadre de ce tutoriel, donc veuillez consulter les ressources pertinentes dans la section Références. Une très bonne introduction que je ne peux recommander assez est l'excellent livre "Statistical Rethinking" de Richard McElreath. Richard McElreath publie également l'ensemble du contenu du livre sous forme de [vidéos sur YouTube](https://www.youtube.com/watch?v=FdnMWdICdRs&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus).

# 3. Structure des données

## Description générale

Pour l'atelier, nous utiliserons une version des données nettoyées et prêtes pour l'analyse qui se trouvent sur [ce dépôt GitHub](https://github.com/quantitative-ecologist/Atelier-CSL-EcotoQ) dans le dossier [data](https://github.com/quantitative-ecologist/Atelier-CSL-EcotoQ/tree/main/data). Ce dépôt a été préparé spécifiquement pour l'atelier.

Voici les spécifications:  

- Le jeu de données servira à faire des modèles linéaires généralisés (GLM) et des modèles linéaires généralisés hiérarchiques (HGLM) univariés
- Nous appliquerons les modèles aux données d'animaux seulement. Nous appelerons le jeu de données `anim`

Importons les données à partir du dépôt GitHub de l'atelier:

```{r donnees}
github <- "https://raw.githubusercontent.com/quantitative-ecologist"
depot <- "Atelier-CSL-EcotoQ"
dossier <- "main/data"

anim <- fread(
  file.path(github, depot, dossier, "animal_dat.csv"),
  stringsAsFactors = TRUE
)
```

## Variables

Une description détaillée des variables du jeu de données est présentée en Annexe de l'article de [Schür et al. (2023)](https://www.nature.com/articles/s41597-023-02612-2).

Dans le cadre de cet atelier, nous utiliserons un échantillon de ces variables.

### Variables sur les résultats expérimentaux (Y)

- `result_effect`: le groupe d'effet de l'expérience, soit, mortalité (`MOR`) exclusivement pour les animaux
- `result_conc_mean`: la concentration effective moyenne en mg/L
- `result_conc_mean_log`: le log de la concentration effective moyenne
- `result_conc1_mean_binary`: si la dose est moins toxique (`0`) ou plus toxique (`1`)

### Variables sur les espèces (X)

- `tax_group`: le groupe taxonomique soit, `fish` ou `crusta`
- `tax_gs`: le nom de l'espèce étudiée codée comme `Genre_épithète`
- `tax_lh_amd`: la longévité de l'espèce en jours

### Variables liées aux conditions expérimentales (X)

- `test_exposure_type`: le type d'exposition, soit, statique (`S`), en flux continu d'eau (`F`), en flux continu d'eau avec renouvellement (`R`), ou non rapporté (`NR`)
- `media_temperature_mean`: la température moyenne du medium en degrés Celsius

Pour l'atelier, nous allons nous intéresser à modéliser les variables liées aux **résultats expérimentaux**. Ce seront nos variables réponse ($Y$). Nous testerons comment ces variables changent en fonction des variables liées aux espèces ainsi qu'aux conditions expérimentales ($X$)

# 4. Exploration des données

## Distribution de la concentration effective

Commençons par évaluer la distribution de la variable réponse, qui est le log de la concentration effective moyenne. Nous allons faire nos graphiques avec la librairie `ggplot2`.

```{r plot1, fig.height=5, fig.width=6, cache=TRUE}
ggplot(data = anim) +
  geom_histogram(
    aes(x = result_conc1_mean_log),
    color = "black", fill = "#440154"
  ) +
  ylab("Compte") +
  xlab("log(concentration moyenne (mg/L))") +
  theme_classic(base_size = 14) +
  theme(panel.grid = element_blank())
```

## Concentration effective selon l'espèce

Vérifions si les concentrations effectives varient selon l'espèce.

```{r plot2, fig.height=7, fig.width=6, cache=TRUE}
ggplot(data = anim) +
  geom_boxplot(
    aes(
      x = result_conc1_mean_log,
      y = tax_gs,
      fill = tax_group
    ),
    color = "black"
  ) +
  ylab("Espèce") +
  xlab("log(concentration moyenne (mg/L))") +
  scale_fill_manual(values = c("#21918c", "#440154")) +
  scale_x_continuous(
    breaks = seq(-8, 8, 4),
    limits = c(-9, 9)
  ) +
  theme_classic(base_size = 14) +
  theme(
    panel.grid = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.position = "top"
  )
```

## Concentration effective selon l'exposition

Finalement, on peut aussi se douter que la concentration effective pourrait changer selon le type d'exposition.

```{r plot4, fig.height=7, fig.width=6, cache=TRUE}
ggplot(data = anim) +
  geom_boxplot(
    aes(
      x = result_conc1_mean_log,
      y = test_exposure_type,
      fill = tax_group
    ),
    color = "black"
  ) +
  ylab("Type d'exposition") +
  xlab("log(concentration moyenne (mg/L))") +
  scale_fill_manual(values = c("#21918c", "#440154")) +
  scale_x_continuous(
    breaks = seq(-8, 8, 4),
    limits = c(-9, 9)
  ) +
  theme_classic(base_size = 14) +
  theme(
    panel.grid = element_blank(),
    legend.position = "top"
  )
```

## Sommaire de l'exploration de données

Les vérifications nous indiquent qu'il semble y avoir une structure hiérarchique dans nos données. En termes simples, cela signifie qu'il y a de la variation dans les concentrations effectives entre les différents groupes que nous avons vu, soit, les espèces ainsi que le type d'exposition.

Les modèles hiérarchiques nous permettront de modéliser cette variation afin d'en tenir compte.

# 5. Rappel sur les modèles linéaires

## Les bases

L'objectif d'un modèle linéaire est de prédire une variable $Y$ tout en minimisant l'écart entre les observations et les prédictions ($\hat{Y}$).

Au travers votre parcours, vous avez sûrement vu différents types de modèles où on utilise des variables explicatives $X_i$ pour prédire une variable $Y$ (ex. ANOVA, régression, ANCOVA). 

Par exemple, une régression linéaire peut s'écrire avec cette équation:

$$Y = X\beta+\epsilon$$

où: 

- $Y$ est le vecteur des valeurs observées (c.-à.-d. la variable réponse)
- $X$ est la matrice de variables explicatives (incluant l'ordonnée à l'origine)
- $\beta$ est le vecteur de coefficients
- $\epsilon$ est le vecteur des résidus

Visuellement, on peut représenter ce modèle avec la figure suivante:

```{r plot5, fig.height=5, fig.width=6, echo=FALSE, cache=TRUE}
# Simulate data
set.seed(123)
n <- 120
rho <- 0.5
x  <- rnorm(n = n, mean = 10, sd = 5)
x <- ifelse(x < 0, 0, x)
y  <- (rho * x) + sqrt(1 - rho * rho) * rnorm(n = n, mean = 10, sd = 5)
y <- ifelse(y < 0, 0, y)

# Combine into a data frame
data <- data.frame(x = x, y = y)

# Fit model and plot
fit <- lm(formula = y ~ x, data = data)
data$predicted <- predict(fit)

# Get the model coefficients
coefficients <- coef(fit)
intercept <- round(coefficients[1], 2)
slope <- round(coefficients[2], 2)
r2 <- format(round(summary(fit)$adj.r.squared, 2), nsmall = 2)

# Create the equation string
equation <- paste0("Y = ", intercept, " + ", slope, " * X")
r2_p <- paste0("R² ajusté = ", r2)

ggplot(data, aes(y = y, x = x)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", formula = y ~ x, colour = "dodgerblue") +
  #geom_abline(intercept = coefficients[1], slope = coefficients[2]) +
  geom_segment(
    aes(xend = x, yend = predicted),
    color = "#6a6a6a",
    linetype = "dashed"
  ) +
  annotate(
    "text",
    x = min(data$x), y = max(data$y),
    label = equation, hjust = 0,
    size = 4, color = "#646262"
  ) +
  annotate(
    "text",
    x = min(data$x), y = max(data$y) - 2,
    label = r2_p, hjust = 0,
    size = 4, color = "#646262"
  ) +
  ylab("Y") +
  xlab("X") +
  scale_y_continuous(
    breaks = seq(0, 32, 8),
    limits = c(0, 34)
  ) +
  scale_x_continuous(
    breaks = seq(0, 20, 5),
    limits = c(0, 21)
  ) +
  theme_classic(base_size = 14) +
  theme(panel.grid = element_blank())
```

Pour interpréter les résultats d'un modèle linéaire, il est essentiel que celui-ci respecte **ces conditions d'application**:  

1. **Linéarité**: la relation entre $Y$ et $X$ est linéaire.
2. **Indépendance**: les observations sont indépendantes l'une de l'autre.
3. **Homoscédasticité**: la variance des résidus est homogène le long des valeurs prédites.
3. **Normalité des résidus**: les résidus suivent une distribution Gaussienne.

## Limites des modèles linéaires

Une des limites majeures des modèles linéaires classiques est qu'ils ne peuvent s'ajuster à des variables réponse pour lesquelles les résidus suivent une distribution non-Gaussienne.

Cette situation est particulièrement commune lorsqu'on analyse des données biologiques, qui ont tendance à suivre d'autres types de distribution telle que de type Poisson ou binomiale.

Dans la section suivante, nous verrons comment les modèles linéaires généralisés nous permettent de palier à ce problème en modélisant spécifiquement la nature des résidus d'une variable $Y$.

# 6. Théorie: les GLM

Dans cette section, nous verrons comment fonctionnent les **modèles linéaires généralisés** (GLM). Nous ferons tout d'abord un survol de trois **familles de distribution**. En les reconnaissant, nous pourrons modéliser nos résidus correctement lorsque nos données suivent une de ces distributions.

Pour y arriver, nous verrons aussi le concept de la **fonction de lien** rattachée à notre famille de distribution. La fonction de lien nous permettra d'estimer la relation linéaire entre $Y$ et $X$ lorsque nos résidus ne sont pas Gaussiens. Nous n'aurons donc pas à transformer nos données pour satisfaire les conditions d'application des modèles linéaires standard.

## Familles de distribution

Il existe une multitude de familles de distribution qui décrivent les variables qu'on peut mesurer. Ici, l'important est de retenir que chaque distribution a des paramètres propres qui décrivent sa forme. Les GLM utilisent ces paramètres pour l'estimation des coefficients, et nous les utiliserons pour l'interprétation de nos modèles.

Voici trois exemples des familles les plus communes.

### Distribution de Poisson

Certaines distributions sont **discrètes** et décrivent des nombres entiers exclusivement, telles que la distribution de **Poisson**. On voit souvent la distribution de Poisson dans les données écologiques comme l'abondance d'espèces ou le nombre d'individus d'une espèce. La distribution de Poisson est décrite par un seul paramètre, $\lambda$, qui décrit sa moyenne et sa variance.

Voici une représentation de cette distribution avec différentes valeur de $\lambda$:

```{r plot6, fig.height=4, fig.width=10, echo=FALSE, cache = TRUE}
# Simulate data for each Poisson distribution
set.seed(123)
lambdas <- c(1, 4, 10)
n <- 1000
poisson <- data.frame(
  x = rep(1:n, times = length(lambdas)),
  lambda = rep(lambdas, each = n),
  y = c(
    rpois(n, lambdas[1]),
    rpois(n, lambdas[2]),
    rpois(n, lambdas[3])
  )
)

# Convert lambda to a factor for plotting purposes
poisson$lambda <- factor(
  poisson$lambda,
  levels = lambdas,
  labels = paste0("lambda = ", lambdas)
)

# Plot the three Poisson distributions using ggplot2
ggplot(
  data = poisson,
  aes(x = y)
) +
  geom_histogram(
    binwidth = 1,
    position = "dodge",
    colour = "black",
    fill = "darkgray"
  ) +
  facet_wrap(~ lambda) +
  labs(
    x = "Valeur",
    y = "Fréquence"
  ) +
  theme_bw(base_size = 14) +
  theme(
    legend.position = "none",
    panel.grid = element_blank()
  )
```

### Distribution de Bernoulli

Une autre distribution communément rencontrée dans les données écologiques est la distribution de Bernoulli. Cette distribution est décrite par le paramètre $p$, étant la probabilité de succès (1) lorsqu'on a données de type 0 ou 1. On observe souvent cette distribution avec des données de présence-absence d'espèces, soit, typiquement lorsqu'une variable n'a que deux issues.

Cette distribution considère l'issue (0, 1) pour une seule expérience ou essai.

Voici une représentation de cette distribution avec différentes valeur de $p$:

```{r plot7, fig.height=4, fig.width=10, echo=FALSE, cache=TRUE}
# Simulate data for each Bernoulli distribution
set.seed(123)
probs <- c(0.1, 0.5, 0.9)
n <- 1000
bernou <- data.table(
  sample = rep(1:n, times = length(probs)),
  probability = rep(probs, each = n),
  value = c(rbinom(n, size = 1, prob = probs[1]), 
            rbinom(n, size = 1, prob = probs[2]), 
            rbinom(n, size = 1, prob = probs[3]))
)

# Calculate the proportions
bernou[, count := .N, by = .(probability, value)]
bernou[, proportion := count / 1000, by = probability]

# Convert probability to a factor for plotting purposes
bernou[
  , probability := factor(
    probability,
    levels = probs,
    labels = paste0("p = ", probs)
  )
]

bernou <- unique(bernou[, .(probability, value, proportion)])

# Plot
ggplot(
  data = bernou,
  aes(x = factor(value), y = proportion)
) +
  geom_bar(
    stat = "identity",
    position = "dodge",
    colour = "black",
    fill = "darkgray"
  ) +
  facet_wrap(~ probability) +
  labs(
    x = "Valeur",
    y = "Probabilité"
  ) +
  scale_y_continuous(breaks = seq(0, 1, 0.25), limits = c(0, 1)) +
  theme_bw(base_size = 14) +
  theme(
    legend.position = "none",
    panel.grid = element_blank()
  )
```

### Distribution binomiale

Finalement, la distribution binomiale est aussi communément observée dans les données biologiques, et consiste en une extension de la distribution de Bernoulli lorsqu'on répète l'essai à plusieurs reprises.

Elle consiste donc en le nombre de succès pour un nombre fixe d'essais indépendants, où chaque essai a la même probabilité de succès $p$. Ses paramètres sont le nombre d'essais $n$ et la probabilité à chaque essai $p$.

Voici une représentation de cette distribution avec différentes valeur de $p$ et un $n$ de 50:

```{r plot8, fig.height=4, fig.width=10, echo=FALSE, cache=TRUE}
# Simulate data for each Binomial distribution
set.seed(123)
probs <- c(0.1, 0.5, 0.9)
n <- 50

binom <- lapply(probs, function(p) {
  data.frame(
    probability = paste0("p = ", p),
    value = rbinom(n, size = n, prob = p)
  )
})
binom <- do.call(rbind, binom)

# Plot
ggplot(binom, aes(x = value)) +
  geom_histogram(
    aes(y = after_stat(density)),
    binwidth = 1, position = "dodge",
    colour = "black",
    fill = "darkgray"
  ) +
  facet_wrap(~ probability) +
  labs(
    x = "Nombre de fois qu'on obtient 1",
    y = "Probabilité"
  ) +
  scale_y_continuous(
    breaks = seq(0, 1, 0.25),
    limits = c(0, 1)
  ) +
  theme_bw(base_size = 14) +
  theme(
    legend.position = "none",
    panel.grid = element_blank()
  )
```

## Fonction de lien

Nous avons vu trois exemples de distributions souvent rencontrées avec des données biologiques.

Dans un GLM, afin que celui-ci puisse estimer la relation entre $Y$ et $X$, on doit donc:

1. Spécifier la distribution des résidus
2. Spécifier la fonction de lien pour les valeurs prédites

Cette deuxième étape consiste à **linéariser la relation** entre $Y$ et $X$. Il s'agit donc de transformer les valeurs prédites $\hat{Y}$ pour avoir une relation linéaire.

Mathématiquement, le prédicteur linéaire dans un GLM est défini comme:

$$\eta = X\beta$$

où:  

- $\beta$ sont les coefficients
- $X$ est la matrice des variables

La fonction de lien transforme le prédicteur linéaire à l'échelle appropriée pour $Y$ tel que:

$$g(\mu) = \eta$$

où: 

- $\mu$ est la moyenne prédite de $Y$ en fonction des prédicteurs $X$
- $g(.)$ est la fonction de lien

Voici une table qui résume les informations relatives à la fonction de lien pour les familles que nous avons vu.

```{r echo = FALSE}
library(knitr)
link_info <- data.frame(
  Distribution = c("Gaussienne", "Poisson", "Bernoulli", "Binomial"),
  Nom = c("Identité", "Log", "Logit", "Logit"),
  Fonction_de_lien = c(
    "$g(\\mu) = \\mu$",
    "$g(\\mu) = \\log(\\mu)$",
    "$g(\\mu) = \\log\\left(\\frac{\\mu}{1 - \\mu}\\right)$",
    "$g(\\mu) = \\log\\left(\\frac{\\mu}{1 - \\mu}\\right)$"
  ),
  Modèle = c(
    "$\\mu = \\mathbf{X} \\boldsymbol{\\beta}$",
    "$\\log(\\mu) = \\mathbf{X} \\boldsymbol{\\beta}$",
    "$\\log\\left(\\frac{\\mu}{1 - \\mu}\\right) = \\mathbf{X} \\boldsymbol{\\beta}$",
    "$\\log\\left(\\frac{\\mu}{1 - \\mu}\\right) = \\mathbf{X} \\boldsymbol{\\beta}$"
  ),
  brms = c(
    "`gaussian(link=\"identity\")`",
    "`poisson(link=\"log\")`",
    "`bernoulli(link=\"logit\")`",
    "`binomial(link=\"logit\")`"
  )
)

# Print the kable table
kable(
  link_info, align = "l",
  escape = FALSE
)
```

# 7. Exemple: GLM avec les données ADORE

Maintenant que nous avons vu les familles de distribution ainsi que la mécanique de la fonction de lien, nous pouvons **estimer les paramètres** d'un GLM avec R.

Nous allons construire un GLM simple avec une variable réponse et trois variables explicatives.

Nos variable sont:

- $Y$: la concentration effective est 1 (plus toxique) ou 0 (moins toxique)
- $X_1$: le type d'exposition
- $X_2$: la température moyenne du medium degrés Celsius
- $X_3$: la longévité de l'espèce en jours

Si on modélisait la relation entre la toxicité de la concentration effective et la température du medium avec une régression linéaire classique, on obtiendrait ceci:

```{r plot9, fig.height=4, fig.width=9, echo=FALSE, cache=TRUE}
fit2 <- lm(
  result_conc1_mean_binary ~ media_temperature_mean,
  data = anim
)

out2 <- data.frame(residuals = resid(fit2), fitted = fitted(fit2))

pex1 <- ggplot(
  data = anim,
  aes(x = media_temperature_mean, y = result_conc1_mean_binary)
) +
  geom_point(alpha = 0.05) +
  geom_abline(
    intercept = fit2$coefficients[1],
    slope = fit2$coefficients[2],
    color = "#440154", linewidth = 1
  ) +
  ylab("Niveau de toxicité") +
  xlab("Température moyenne du médium") +
  scale_y_continuous(
    breaks = seq(0, 1, 0.25),
    limits = c(0, 1)
  ) +
  theme_classic(base_size = 14) +
  theme(panel.grid = element_blank())

pex2 <- ggplot(
  data = out2,
  aes(x = fitted, y = residuals)
) +
  geom_point(alpha = 0.05) +
  geom_abline(
    intercept = 0,
    slope = 0,
    linewidth = 1,
    linetype = "dashed",
    color = "#440154"
  ) +
  ylab("Résidus normalisés") +
  xlab("Valeurs prédites") +
  scale_y_continuous(
    breaks = seq(-0.8, 0.8, 0.4),
    limits = c(-0.8, 0.8)
  ) +
  theme_classic(base_size = 14) +
  theme(panel.grid = element_blank())

ggarrange(pex1, pex2)
```

Comme vous pouvez voir dans le panneau à gauche, le modèle ne décrit pas bien la relation entre le niveau de toxicité et la température. De plus, il aurait pu y avoir des cas où la droite dépasse le seuil possible de nos valeurs (ici 0 ou 1).

On peut voir à gauche que les résidus de notre modèle ne respectent pas la condition d'homoscédasticité.

Pour ces raisons, on doit donc établir une limite inférieure et supérieure à nos données en **spécifiant une distribution de Bernoulli** pour modéliser la toxicité de la concentration effective.

## Préparation des données

Commençons par évaluer la structure des variables avant de faire le modèle.

```{r}
str(anim[
  , .(
    result_conc1_mean_binary, test_exposure_type,
    tax_lh_amd, media_temperature_mean
  )
])
```

Les données sont dans le format adéquat pour la modélisation.

Avans de faire le modèle, nous allons **normaliser les variables explicatives continues** en les ramenant à la même échelle. Ceci peut faciliter les calculs des coefficients et simplifier l'interprétation des paramètres.

```{r}
# Fonction de normalisation en score Z.
standardize <- function(x) {
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}

# Créer deux nouvelles colones
anim[
  , c("Ztax_lh_amd", "Zmedia_temperature_mean") := lapply(
    .SD,
    standardize
  )
  , .SDcols = c("tax_lh_amd", "media_temperature_mean")
]
```

## Construction du modèle

Dans cet exemple, on s'intéresse à estimer comment la toxicité de la concentration effective varie selon le type d'exposition, la longévité moyenne de l'espèce, et la température du médium durant l'expérience.

Voici l'équation de notre modèle:

$$\eta = X\beta + Z\gamma + 1\beta_0 + \epsilon$$

où:

- $\eta$ est le prédicteur linéaire
- $X$ est la matrice de conception du type d'exposition
- $\beta$ est le vecteur des coefficients du type d'exposition
- $Z$ est la matrice de conception des prédicteurs linéaires (longévité et température)
- $\gamma$ est le vecteur des coefficients du type d'exposition des prédicteurs linéaires
- $1$ est le vecteur de 1 pour l'ordonnée à l'origine
- $\beta_0$ est l'ordonnée à l'origine pour le niveau de référence

La fonction de lien du modèle est:

$$logit(p) = log(\frac{p}{1-p})$$

où:

$p$ est la probabilité que la concentration effective soit toxique (1) pour chaque observation. 

On estime $p$ en utilisant l'inverse de la fonction logit:

$$p = \text{logit}^{-1}(\eta) = \frac{1}{1 + e^{-\eta}}$$

Spécifions ce modèle dans `brms` avec la fonction `brmsformula()`:

```{r}
form1 <- brmsformula(
  result_conc1_mean_binary ~
  test_exposure_type +
  Ztax_lh_amd +
  Zmedia_temperature_mean
)
```

## Informations à priori

Comme nous l'avons vu dans la section sur l'inférence Bayésienne, la distribution postérieure est estimée en multipliant la distribution à priori et la distribution de vraisemblance des données (les preuves).

Par défaut, `brms` emploie des distributions à priori très larges qui ne réflètent pas nos hypothèses/connaissances sur notre système. Définir les connaissances à priori est donc très important en inférence Bayésienne, et je vous suggère fortement de faire ce processus. C'est ce qui différencie l'inférence Bayésienne de l'approche fréquentiste.

Ici, puisque je ne connais pas assez bien l'écotoxicologie, je spécifie des "priors" relativement informatifs suivant une distribution Gaussienne avec une moyenne de 0 et un écart-type de 1.

Cela signifie que je crois que les valeurs des paramètres devraient se situer autour de 0 avec des valeurs positives allant de +1 à -1 écart-types.

Les informations à priori à spécifier sont:

- sur l'ordonnée à l'origine globale (la moyenne du type d'exposition `F`)
- sur les différences de moyenne par rapport à `F`
- sur les pentes pour la température et la longévité

Pour vous aider à comprendre, imaginez que vous savez avec la littérature que la dose effective devrait être plus toxique avec une augmentation de la température. Ainsi, pour la pente de ce paramètre, vous pourriez spécifier une distribution Gaussienne avec une moyenne de 1 et un écart-type de 0.5. Vous diriez donc à votre modèle que vous assumez que cette valeur devrait être positive, se situant autour de 1, avec peu de variation autour de cette valeur.

```{r}
priors1 <- c(
  # Informations à priori sur l'ordonnée à l'origine
  set_prior(
    "normal(0, 1)",
    class = "Intercept"
  ),
  # Informations à priori sur les différences de moyenne
  set_prior(
    "normal(0, 1)",
    class = "b",
    coef = c(
      "test_exposure_typeNR",
      "test_exposure_typeR",
      "test_exposure_typeS"
    )
  ),
  # Informations à priori sur les pentes
  set_prior(
    "normal(0, 1)",
    class = "b",
    coef = c(
      "Zmedia_temperature_mean",
      "Ztax_lh_amd"
    )
  )
)
```

## Estimation des paramètres

### Spécifications pour l'estimation MCMC

À titre de rappel, `brms` utilise l'algorithme de HMC-NUTS pour estimer les paramètres du modèle.

L'agorithme va effectuer un nombre $n$ d'itérations que nous allons spécifier. À chaque itération, l'algorithme estimera la valeur des paramètres d'intérêt de notre modèle. 

Généralement, on effectue $n$ itérations sur **4 chaînes de Markov** qu'on combine pour avoir les distributions postérieures. Pour une bonne précision, on voit souvent des distributions postérieures avec $\ge 1000$  échantillons au total.

Voici les spécifications pour l'estimation des paramètres par HMC-NUTS. Pour chacune des 4 chaînes, nous aurons:

- n: 2000
- réchauffement: n / 2
- itérations: n - réchauffement
- échantillonnage: itérations / échantillonnage = 1000

Notre modèle fera donc 1000 réchauffements (`warmup = 1000`), 1000 itérations (`iter = 2000` où 2000-1000), et échantillonera une valeur de paramètre à chaque itération (`thin = 1`).

Nous obtiendrons ainsi 1000 échantillons par chaîne que nous combinerons pour un total de **4000 échantillons postérieurs par paramètre**.

### Accélérer l'estimation (facultatif)

Il est à noter qu'estimer un modèle par MCMC peut prendre beaucoup de temps, particulièrement lorsqu'il y a beaucoup de données et qu'on veut faire beaucoup d'itérations.

Pour cette raison, vous pouvez tirer profit des coeurs du processeur de votre ordinateur pour accélérer l'estimation.

Nous allons donc vérifier le nombre de coeurs que nous avons.

```{r}
parallel::detectCores()
```

La fonction indique que j'ai 16 coeurs disponibles dans mon ordinateur. On doit toujours en réserver un pour les processus internes.

**Si vous en avez seulement 4 ou moins, passez à la section "Si vous avez $\le4$ coeurs"**

#### Si vous avez $\ge 6$ coeurs

Je vais utiliser 12 coeurs qui effectueront un processus parallèle sur 4 chaînes.

Je spécifie 12 et non 14 car 14 / 4 = 3.5, un chiffre impair. On doit toujours avoir un nombre de coeurs pairs pour effectuer les tâches en parallèle.

Puisque j'utiliserai 12 coeurs en parallèle sur 4 chaînes, 3 coeurs seront sollicités par chaîne. Ceci est spécificé dans l'argument `threads = threading(3)`.

#### Si vous avez $\le 4$ coeurs

Si vous avez 4 coeurs ou moins sur votre ordinateur, vous ne pourrez pas effectuer le modèle en parallèle. Vous devrez ainsi estimer les paramètres un échantillon et une chaîne à la fois, ce qui prendra plus de temps.

Toutefois, il est possible que 3 chaînes effectuent les calculs simultanément si vous avez 4 coeurs. Vous pouvez ainsi remplacer l'argument `threads = threading(3)` par `cores = parallel::detectCores()-1`. Vous utiliserez donc 3 coeurs pour que 3 chaînes puissent échantilloner les paramètres simultanément, un échantillon à la fois.

Ultimement, si vous avez un gros modèle à faire rouler, je suggère de l'effectuer sur un serveur de calcul si vous y avez accès.

### Exécution de l'estimateur

Nous sommes donc prêt.e.s à estimer les paramètres:

```{r glm1, cache=TRUE}
glm1 <- brm(

  # La formule spécifiée
  formula = form1,

  # La famille de distribution
  family = bernoulli(link = "logit"),

  # Les paramètres d'itérations pour le MCMC
  iter = 2000,
  warmup = 1000,
  thin = 1,
  chains = 4,

  # Paramètres pour accélérer les calculs
  # Utiliser seulement si vous avez >4 coeurs
  threads = threading(3),

  # Si vous avez 4 coeurs ou moins
  # cores = parallel::detectCores()-1,

  # Estimateur
  backend = "cmdstanr",

  # Reproductibilité
  seed = 123,

  # Paramètres de contrôle
  control = list(adapt_delta = 0.95),

  # Distributions à priori
  prior = priors1,

  # Échantillonage des distributions à priori
  # Ceci peut être utile à des fins de comparaison
  sample_prior = TRUE,

  # Données
  data = anim
)
```

Puisque l'estimation peut prendre quelques temps, on créé un dossier où nous allons sauvegarder le modèle.

```{r}
chemin <- file.path(getwd(), "outputs")
dir.create(chemin)
```

On sauvegarde le modèle en format `.rds` qui pourra facilement être réimporté dans la session au besoin.

```{r}
saveRDS(
  object = glm1,
  file = file.path(chemin, "glm1.rds")
)
```

## Vérifications du modèle

Maintenant que notre modèle a terminé son exécution, il est essentiel de faire des inspections pour s'assurer que les paramètres estimés sont fiables. Ces étapes d'inspections sont cruciales pour s'assurer que nos interprétations biologiques font du sens. Autrement, nous pourrions avoir de mauvaises surprises.

Commençons par extraire les échantillons des distributions à posteriori dans un `data.frame` pour les manipuler.

```{r}
params1 <- as_draws_df(
  x = glm1, add_chain = TRUE,
  regex = TRUE
)
```

### Convergence des chaînes

La première chose que nous allons faire est de vérifier si les chaînes de Markov ont bien convergé pour chacun de nos paramètres d'intérêt. Nous allons faire ceci en inspectant des "trace plots".

Nous pouvons déduire que les chaînes ont convergé si les "trace plots" sont homogènes.

```{r plot10, cache=TRUE}
pars1 <- c(
  "b_Intercept", "b_test_exposure_typeNR", "b_test_exposure_typeR",
  "b_test_exposure_typeS", "b_Ztax_lh_amd", "b_Zmedia_temperature_mean"
)

bayesplot::mcmc_trace(
  params1,
  pars = pars1
)
```

Les chaînes de notre modèle semblent avoir très bien convergé.

Une autre étape d'inspection de convergence des chaînes de Markov est d'évaluer les **$\hat{R}$** et les **tailles d'échantillon effectives** pour chaque paramètre d'intérêt.

- Dans @Burkner2017, on peut lire que la taille d'échantillon effective est: "le nombre d'échantillons indépendants de la distribution postérieure qui serait attendu pour produire la même erreur standard de la moyenne postérieure que celle obtenue à partir des échantillons dépendants retournés par l'algorithme MCMC". Dans `brms`, le résumé du modèle fournit une estimation des tailles d'échantillon effectives de la partie centrale et des queues de la distribution postérieure. Le `Bulk_ESS` rapporte l'efficacité de l'échantillonnage de la partie centrale de la distribution postérieure (c'est-à-dire la médiane ou la moyenne), tandis que le `Tail_ESS` diagnostique l'efficacité de l'échantillonnage des queues de la distribution postérieure (c'est-à-dire les quantiles 5% et 95%). Nous utilisons un seuil de <100 comme règle de décision pour savoir si les chaînes ont convergé pour les paramètres [@Vehtari.etal2021a].

- En revanche, le $\hat{R}$ est un outil de diagnostic quantitatif courant pour savoir si les chaînes ont convergé [@gelmanBayesianDataAnalysis2013;@Burkner2017]. Il compare la variance à l'intérieur d'une chaîne à la variance entre les chaînes. Différentes versions et seuils ont été proposés pour cette mesure [@gelmanInferenceIterativeSimulation1992;@gelmanBayesianDataAnalysis2013;@Vehtari.etal2021a], mais nous utiliserons la plus parcimonieuse suggérée par @Vehtari.etal2021a avec un seuil fixé à <1.01.

Dans brms, le $\hat{R}$, le `Bulk_ESS`, et le `Tail_ESS` apparaissent toujours à côté des moyennes postérieures des paramètres affichées avec la fonction `summary()`. Examinons les valeurs pour nos paramètres:

```{r}
glm1_t <- data.table(
  round(summary(glm1, prob = 0.89, robust = TRUE)$fixed, digits = 3),
  keep.rownames = TRUE
)
setnames(glm1_t, old = "rn", new = "Parameter")

glm1_t[, c(1, 6:8)]
```

Les valeurs sont très bien!

### Ajustement du modèle

Nous allons maintenant examiner si notre modèle a bien prédit les données en utilisant les [vérifications prédictives postérieures](https://mc-stan.org/bayesplot/reference/PPC-overview.html). Les vérifications prédictives postérieures utilisent des données simulées à partir de la distribution prédictive postérieure d'un modèle et les comparent aux données brutes utilisées pour ajuster le modèle. @gabryVisualizationBayesianWorkflow2019 suggèrent qu'un modèle bien ajusté devrait générer des données ressemblant aux données qui ont été observées (bruts). Ici, $y$ est la distribution de nos données observées, et $y_{rep}$ est la distribution des données simulées à partir de la distribution prédictive postérieure du modèle.

La production des graphiques peut prendre du temps en fonction de la complexité du modèle et de la taille des données. Vous pouvez spécifier le nombre d'échantillons à afficher en utilisant l'argument `ndraws` dans la fonction `pp_check()` pour accélérer le processus.

```{r plot11, fig.height=5, fig.width=6, cache=TRUE}
pp_check(glm1, ndraws = 100)
```

Notre modèle semble très bien ajusté aux données!

En somme, nos inspections indiquent que le modèle est très bon. Toutefois, je vous encourage à consulter la section sur les inspections additionnelles pour mieux diagnostiquer vos modèles.

## Résultats

### Sommaire des paramètres

Par défaut, la fonction `summary()` dans `brms` rapporte la moyenne de la distribution postérieure avec les intervales de compatibilité inférieurs et supérieurs à 95%.

Toutefois, il est suggéré en analyse Bayésienne de rapporter les intervalles à 89% car ils procurent davantage de stabilité. De plus, la médiane est une meilleure mesure de tendance centrale lorsque la distribution postérieure n'est pas symétrique. 

Nous avons donc ajouté des arguments à la fonction summary précédemment en faisant ce code:

```{r, eval=FALSE}
glm1_t <- data.table(
  round(summary(glm1, prob = 0.89, robust = TRUE)$fixed, digits = 3),
  keep.rownames = TRUE
)
setnames(glm1_t, old = "rn", new = "Parameter")
```

où `robust = TRUE` indique de rapporter la médiane, et `prop=0.89` les intervalles à 89%. Au final, la décision de l'intervalle et arbitraire, et le mieux est de visualiser la distribution entière de l'estimé. Nous ferons ceci plus tard.

Visualisons la table de sommaire:

```{r}
knitr::kable(glm1_t[, c(1, 2, 4, 5)])
```

On peut voir que:

- Il y a des différences de toxicité selon le type d'exposition
- La probabilité que la CL50 soit plus toxique augmente avec la longévité
- La probabilité que la CL50 soit plus toxique diminue avec la température moyenne

Nous allons visualiser ces résultats avec des figures.

### Figures

#### Calcul des prédictions

Tout d'abord, on extrait les prédictions pour la longévité moyenne ainsi que la température moyenne avec la fonction `conditional_effects()` qui est spécifique à `brms`.

Quelques spécifications:

- l'argument `spaghetti=` va chercher des prédictions posterieures
- l'argument `ndraws=` spécifie le nombre de prédictions postérieures (ici, 100)
- l'argument `robust=` spécifie qu'on veut la médiane de la distribution postérieure des prédictions
- pour chacune des 100 droites, nous aurons 100 valeurs. C'est le paramètre par défaut de `conditional_effects()`

Il est possible de tracer le graphique directement à partir de la fonction. Toutefois, nous allons plutôt conserver les valeurs dans une table afin d'avoir plus de flexibilité sur les paramètres graphiques.

**Température**

```{r}
tab1 <- conditional_effects(
  x = glm1, method = "posterior_epred",
  effects = "Zmedia_temperature_mean",
  spaghetti = TRUE, ndraws = 100,
  robust = TRUE, prob = 0.89
)
spag1 <- attr(tab1$Zmedia_temperature_mean, "spaghetti")
tab1 <- data.table(tab1[[1]])
```

**Longévité**

```{r}
tab2 <- conditional_effects(
  x = glm1, method = "posterior_epred",
  effects = "Ztax_lh_amd",
  spaghetti = TRUE, ndraws = 100,
  robust = TRUE, prob = 0.89
)
spag2 <- attr(tab2$Ztax_lh_amd, "spaghetti")
tab2 <- data.table(tab2[[1]])
```

**Type d'exposition**

Ici, on va utiliser les distributions postérieures des moyennes prédites de chaque type d'exposition. Nous avons déjà extrait ceci dans une étape ultérieure et assigné à l'objet `params`. 

Cela nous permettra de comparer les moyennes entre chaque type d'exposition avec une approche Bayésienne.

Visualisons l'objet:

```{r}
params1
```

Nous allons utiliser les 4 premières colones pour produire le graphique. Ces 4 colones sont:

- `b_Intercept`: la moyenne de l'exposition de type `F`
- `b_test_exposure_typeNR` est la différence entre `NR` et `F`
- `b_test_exposure_typeR` est la différentre entre `R` et `F`
- `b_test_exposure_typeS` est la différentre entre `S` et `F`

Nous devons manipuler un peu la table pour faire le graphique.

On veut:  

1. calculer la moyenne pour les expositions `NR`, `R`, et `S`
2. ramener les valeurs à une échelle de probabilité
3. calculer la médiane des distributions postérieures

Commençons par **l'étape 1**.

```{r}
# Transformer en data.table pour manipuler
paramsdt1 <- data.table(params1)

# On calcule les vraies moyennes
paramsdt1[, c(2:4) := paramsdt1[, c(2:4)] + b_Intercept]
```

Ensuite **l'étape 2**.

Ici, souvenez-vous que la fonction de lien nous a permis d'estimer un modèle sur une échelle linéaire. Toutefois, cette échelle est difficilement interprétable. On doit donc ramener les prédictions à l'échelle de notre variable, soit, en probabilités.

```{r}
# Changer en format long
preds1 <- melt(
  data = paramsdt1[, c(1:4)],
  variable.name = "exposure_type",
  measure = colnames(paramsdt1[, c(1:4)])
)

# Ramener à l'échelle de probabilité
preds1[, value := plogis(value)]
```

Finalement, **l'étape 3**.

```{r}
# Calculer les médianes
medians1 <- preds1[, .(median = median(value)), by = exposure_type]
```

#### Visualisation

On peut maintenant produire les graphiques. À titre de rappel, on a la même pente pour l'ensemble des types d'exposition, et des ordonnées à l'origine qui diffèrent (c-à-d, les moyennes de toxicité diffèrent). Ici, la droite correspond au type d'exposition `F`. 

Commençons avec les différences de moyennes.

```{r plot12, fig.height=5, fig.width=6, cache=TRUE}
ggplot(
  data = preds1,
  aes(x = value, fill = exposure_type)
) +
  geom_density(alpha = 0.5, colour = "black") +
  geom_vline(
    data = medians1,
    aes(xintercept = median, color = exposure_type),
    linetype = "dashed"
  ) +
  scale_fill_manual(
    labels = c(
      "Flux", "NS",
      "Renouvellement", "Statique"
    ),
    values = c("#440154", "#3d4d8a", "#24878e", "#40bd72")
  ) +
  scale_colour_manual(
    labels = c(
      "Flux", "NS",
      "Renouvellement", "Statique"
    ),
    values = c("#440154", "#3d4d8a", "#24878e", "#40bd72")
  ) +
  #scale_x_continuous(breaks = seq(0.25, 1, 0.25), limits = c(0.25, 1)) +
  xlab("\nProbabilité d'être plus toxique") +
  ylab("Densité\n") +
  labs(fill = "Exposition", colour = "Exposition") +
  theme_classic(base_size = 14) +
  theme(legend.position = "top")
```

Finalement, les droites pour la température et la longévité pour le type d'exposition `F`.

```{r plot13, fig.height=4, fig.width=8}
glm_p1 <- ggplot() +
  geom_point(
    data = anim,
    aes(
      y = result_conc1_mean_binary,
      x = Zmedia_temperature_mean
    ),
    alpha = 0.05
  ) +
  geom_line(
    data = spag1,
    aes(
      x = Zmedia_temperature_mean,
      y = estimate__,
      group = sample__
    ),
    linewidth = 1, alpha = 0.05,
    colour = "#440154"
  ) +
  geom_line(
    data = tab1,
    aes(
      x = Zmedia_temperature_mean,
      y = estimate__
    ),
    linewidth = 0.5,
    colour = "yellow"
  ) +
  ylab("Toxicité\n") +
  scale_y_continuous(
    breaks = seq(0, 1, 0.25),
    limits = c(0, 1)
  ) +
  xlab("\nTempérature moyenne normalisée") +
  theme_classic(base_size = 14)

glm_p2 <- ggplot() +
  geom_point(
    data = anim,
    aes(
      y = result_conc1_mean_binary,
      x = Ztax_lh_amd
    ),
    alpha = 0.05
  ) +
  geom_line(
    data = spag2,
    aes(
      x = Ztax_lh_amd,
      y = estimate__,
      group = sample__
    ),
    linewidth = 1, alpha = 0.05,
    colour = "#440154"
  ) +
  geom_line(
    data = tab2,
    aes(
      x = Ztax_lh_amd,
      y = estimate__
    ),
    linewidth = 0.5,
    colour = "yellow"
  ) +
  ylab("Toxicité\n") +
  scale_y_continuous(
    breaks = seq(0, 1, 0.25),
    limits = c(0, 1)
  ) +
  xlab("\nLongévité moyenne normalisée") +
  theme_classic(base_size = 14)

ggarrange(glm_p1, glm_p2)
```

## Interprétations

Nos résultats montrent que:

1. Il y a 40% de chances que la concentration effective soit plus toxique lorsque les conditions expérimentales utilisent un flux d'eau continu. Nous avons de bonnes preuves que ce pourcentage ne diffère pas entre un traitement avec renouvellement. Nous avons également de fortes preuves que des conditions statiques augmentent les changes que la concentration effective soit toxique par rapport à un flux continu. Les expériences où le traitement n'est pas spécifié ont les plus hautes chances que la concentration effective soit toxique. 

2. Nous avons de fortes preuves que la probabilité que la concentration effective soit toxique diminue avec la température du médium. Les chances que la concentration soit toxique diminuent d'environ 25% des plus basses aux plus hautes températures.

3. Nous avons de fortes preuves que la probabilité que la concentration effective soit toxique augmente avec la longévité moyenne de l'espèce. À partir des espèces à courte durée de vie, on observe une augmentation d'environ 10% à 15% dans la probabilité pour les espèces à plus haute longévité.

# 8. Théorie: les HGLM

## Complexité des données biologiques

En explorant nos données, nous avons vu qu'il semble y avoir une structure hiérarchique, où la concentration effective varie entre les espèces.

Ceci est un exemple concret démontrant comment les données biologique sont souvent complexes, et structurées à différentes échelles.

Reprenons l'exemple de la concentration effective en fonction de la température du médium. Cette fois, on ajoute les espèces dans la figure. Ici, chaque couleur correspond à un taxon différent.

```{r plot14, fig.height=5, fig.width=6, echo =FALSE}
ggplot(
  data = anim,
  aes(
    x = media_temperature_mean,
    y = result_conc1_mean_log,
    fill = tax_gs,
    colour = tax_gs
  )
) +
  xlab("Température moyenne en degrés Celsius") +
  ylab("log(concentration moyenne (mg/L))") +
  scale_colour_viridis(option = "D", discrete = TRUE) +
  geom_point(alpha = 0.25) +
  theme_classic(base_size = 14) +
  theme(legend.position = "none")
```

À partir de ce graphique, on peut se poser deux questions:  

1. Est-ce que les taxons ont la même concentration effective moyenne?
2. Est-ce que la relation entre la concentration effective et la température change selon les taxons?

Pour la première question, notre réflexe pourrait être de faire le même GLM que nous avons fait à la section précédente, et d'ajouter le taxon comme variable catégorique.

Pour la seconde question, on pourrait spécifier un modèle de type ANCOVA, en modélisant une interaction entre la température et le taxon.

Pour répondre aux deux questions, on aurait ce modèle:

```{r, eval=FALSE}
result_conc1_mean_log ~
  test_exposure_type +
  Ztax_lh_amd +
  Zmedia_temperature_mean +
  tax_gs +
  Zmedia_temperature_mean:tax_gs
```

Toutefois, on se retrouve avec un gros problème ici. Visualisons-le:

```{r plot15, fig.height=5, fig.width=6, echo = FALSE}
ggplot(
  data = anim,
  aes(
    x = media_temperature_mean,
    y = result_conc1_mean_log,
    fill = tax_gs,
    colour = tax_gs
  )
) +
  geom_point(alpha = 0.25) +
  geom_smooth(method = lm, se = FALSE) +
  scale_colour_viridis(option = "D", discrete = TRUE) +
  xlab("Température moyenne en degrés Celsius") +
  ylab("log(concentration moyenne (mg/L))") +
  theme_classic(base_size = 14) +
  theme(legend.position = "none")
```

Nous avons 105 espèces, et donc 105 droites à estimer. Cela signifie que le modèle doit estimer:

- 105 ordonnées à l'origine
- 105 pentes

C'est donc **210 paramètres de + à estimer!** Si on utilisait l'approche fréquentiste pour l'analyse de données, ça nous coûterait cher en degrés de liberté. Ceci est moins problématique pour l'approche Bayésienne car on n'utilise pas la *valeur p*.

Fréquentiste ou Bayésien, le plus important, c'est qu'en estimant 210 paramètres, la sortie de la fonction `summary()` serait cauchemardesque. Pensez-y! Plus haut, nous avions un `Intercept_b` qui utilisait le type d'exposition `F` comme référence. Ensuite, le modèle calculait la différence entre les autres types d'exposition et `F`. Je vous laisse imaginer celà avec 105 taxons...

## L'avantage des modèles hiérarchiques

Les modèles hiérarchiques nous permettent de palier aux problèmes que nous avons vu plus haut:

- permettent de séparer des effets populationnels d'effets groupés
- permettent de modéliser la structure hiérarchique de nos données en **réduisant le nombre de paramètres à estimer**
- exploitent les paramètres populationnels pour estimer les paramètres de groupe même quand la taille d'échantillon est faible pour un groupe
- tiennent compte de la nonindépendance des échantillons (ex. nous avons plusieurs observations par espèce dans nos données)

Ainsi, les modèles hiérarchiques permettent de modéliser des facteurs associés à la structure de vos expériences. Voici quelques exemples:

- On peut intégrer les quadrats, ou les sites
- On peut ajouter les individus si des données sont prises à répétition
- Permettent de considérer l'aspect temporel dans les données

Nous verrons comment ces principes fonctionnent dans la prochaine section avec les concepts d'**effets fixes** et d'**effets aléatoires**.

## Effets fixes et aléatoires

Reprenons l'exemple du modèle que nous avons construit précédemment.

On s'intéresse toujours à estimer comment la concentration effective ($Y$) varie selon le type d'exposition, la longévité moyenne de l'espèce, et la température du médium durant l'expérience ($X$). On ajoute toutefois **les espèces comme effet aléatoire** ($U$).

Ici, on veut donc savoir si les espèces varient dans leur concentration effective moyenne, et si la relation entre celle-ci et la température change d'une espèce à l'autre.

Voici l'équation de notre modèle:

$$\eta = X\beta + Z\gamma + U_1\delta_1 + U_2\delta_2 + 1\beta_0 + \epsilon$$

où les paramètres ajoutés sont:

- $U_1$ est la matrice de conception pour les ordonnées à l'origine aléatoires
- $\delta_1$ est le vecteur des ordonnées à l'origine aléatoires des espèces
- $U_2$ est la matrice de conception pour les pentes aléatoires
- $\delta_2$ est le vecteur des pentes aléatoires des espèces

**Les effets fixes**, représentés par les coefficients $\beta$ et $\gamma$, modélisent les effets moyens des variables explicatives sur la variable réponse. Ils sont constants pour toutes les unités observées (c.-à.-d., les espèces par exemple). Ce sont les effets que nous avons estimés précédemment dans notre GLM. Celà ne change donc pas. Les effets fixes peuvent être des variables catégoriques **ou** des variables continues.

**Les effets aléatoires**, quant à eux, représentés par les coefficients $\delta_1$ et $\delta_2$, modélisent la variation entre les différents niveaux d'une variable catégorique. Dans le contexte de notre exemple, on intègre les différences entre les espèces en termes de sensibilité à la concentration en ajoutant un effet aléatoire. Les effets aléatoires sont **strictement** des variables catégoriques

En ajoutant cet effet aléatoire, le modèle peut capturer une part supplémentaire de la variation de $Y$ qui ne peut pas être expliquée par les effets fixes. Cela permet de mieux ajuster le modèle aux données et d'améliorer les prédictions.

## Les types de modèles hiérarchiques

Il existe **deux types de modèles hiérarchiques** correspondant aux effets aléatoires estimés.

### Modèle d'ordonnées à l'origine aléatoires

Le premier type de modèle est le **modèle d'ordonnées à l'origine aléatoires**. Ce modèle assume que la relation entre $Y$ et $X$ est la même pour l'ensemble de nos groupes, mais que la moyenne de $Y$ varie entre les groupes. C'est donc un modèle où l'on estime une moyenne pour chacun des niveaux de notre facteur d'intérêt, mais pour lequel la relation entre $Y$ et $X$ est la même que l'effet fixe pour l'ensemble des niveaux du facteur.

Voici à quoi ressemble ce type de modèle pour des données hiérarchiques.

On peut voir que :

- chaque groupe a une ordonnée à l'origine différente (c.-à-d., une moyenne différente)
- tous les groupes on la même pente, suivant la tendance populationnelle (droite pointillée)
- pour l'effet aléatoire, nous avons **1 paramètre** étant la **variance entre les ordonnées à l'origine**

```{r include=FALSE}
library(lme4)

# Set seed for reproducibility
set.seed(123)

# Number of observations per group
n_per_group <- 200

# Create a data frame with grouping factor 'group'
data <- data.frame(
  x = rep(rnorm(n_per_group), 6),
  group = rep(letters[1:6], each = n_per_group)
)

# Function to generate y based on x and group
generate_y <- function(x, group) {
  if (group == "a") {
    return(2 * x + rnorm(length(x), mean = 0, sd = 3))
  } else if (group == "b") {
    return(3 * x + rnorm(length(x), mean = 4, sd = 2))
  } else if (group == "c") {
    return(4 * x + rnorm(length(x), mean = 2, sd = 1))
  } else if (group == "d") {
    return(5 * x + rnorm(length(x), mean = -1, sd = 3))
  } else if (group == "e") {
    return(6 * x + rnorm(length(x), mean = -2, sd = 2))
  } else {
    return(7 * x + rnorm(length(x), mean = -6, sd = 3))
  }
}

# Generate y based on x and group
data$y <- mapply(generate_y, data$x, data$group)
data$group <- as.factor(data$group)

# Fit models
lm_intercepts <- lmer(y ~ x + (1 | group), data = data)
lm_slopes <- lmer(y ~ x + (1 + x | group), data = data)

# Extract BLUPs
blups_intercepts <- coef(lm_intercepts)$group
blups_slopes <- coef(lm_slopes)$group

# Combine into a single dataframe
blups_intercepts <- data.frame(
  group = rownames(blups_intercepts),
  intercept = as.numeric(blups_intercepts[, 1]),
  slope = as.numeric(blups_intercepts[, 2])
)

blups_slopes <- data.frame(
  group = rownames(blups_slopes),
  intercept = as.numeric(blups_slopes[, 1]),
  slope = as.numeric(blups_slopes[, 2])
)

blups <- rbind(
  blups_intercepts,
  blups_slopes
)
blups <- cbind(
  blups,
  model = c(
    rep("Ordonnées à l'origine aléatoires", 6),
    rep("Pentes aléatoires", 6)
  )
)
```

```{r plot16, fig.height=5, fig.width=6, echo = FALSE}
# Plot the data
ggplot() +
  geom_point(
    data = data,
    aes(x = x, y = y, colour = group),
    alpha = 0.25
  ) +
  geom_abline(
    data = blups[1:6, ],
    aes(intercept = intercept, slope = slope, colour = group)
  ) +
  geom_abline(
    intercept = fixef(lm_intercepts)[1],
    slope = fixef(lm_intercepts)[2],
    linewidth = 1,
    linetype = "dashed",
    colour = "black"
  ) +
  ylab("Y") +
  xlab("X") +
  theme_classic(base_size = 14)
```

### Modèle de pentes aléatoires

Le **second type** de modèle est construit à partir de la mécanique du modèle d'ordonnées à l'origine aléatoires. C'est le **modèle de pentes aléatoires**. Comme le nom l'indique, c'est un modèle où chaque niveau du facteur peut maintenant avoir sa propre relation entre $Y$ et $X$.

Voici à quoi ressemble ce type de modèle pour des données hiérarchiques.

On peut voir que :

- chaque groupe a une ordonnée à l'origine différente (c.à-d., une moyenne différente)
- chaque groupe a une pente différente (c.-à-d., une relation propre entre $Y$ et $X$)
- pour l'effet aléatoire, nous avons **2 paramètres** étant la **variance entre les ordonnées à l'origine** et la **variance entre les pentes**

```{r plot17, fig.height=5, fig.width=6, echo = FALSE}
# Plot the data
ggplot() +
  geom_point(
    data = data,
    aes(x = x, y = y, colour = group),
    alpha = 0.25
  ) +
  geom_abline(
    data = blups[7:12, ],
    aes(intercept = intercept, slope = slope, colour = group)
  ) +
  geom_abline(
    intercept = fixef(lm_slopes)[1],
    slope = fixef(lm_slopes)[2],
    linewidth = 1,
    linetype = "dashed",
    colour = "black"
  ) +
  ylab("Y") +
  xlab("X") +
  theme_classic(base_size = 14)
```

Comme nous l'avons soulevé plus haut, un avantage majeur des modèles hiérarchiques est qu'ils réduisent le nombre de paramètres à estimer. Au début de cette section, nous avons vu que si nous faisions une ANCOVA, nous aurions 210 paramètres à estimer. En exploitant les effets aléatoires **nous réduisons ce nombre de paramètres à 2 seulement!**

La raison est qu'au lieu d'estimer la différence d'ordonnée à l'origine pour chaque niveau du facteur ainsi que la différence de pente, un modèle hiérarchique va plutôt calculer chaque valeur pour chaque niveau (c.-à-d., moyenne et pente), et ensuite **estimer une variance**.

On a donc: 

- la variance entre les ordonnées à l'origine (c.-à-d., les moyennes des niveaux du facteur)
- la variance entre les pentes (c.-à-d., la relation entre $Y$ et $X$ pour chaque niveau du facteur)

Ce que la variance nous dit:  

- une variance faible entre les ordonnées à l'origine = la moyenne de $Y$ de chaque espèce est semblable
- une variance faible entre les pentes = la relation entre $Y$ et $X$ pour chaque espèce est semblable

Nous verrons celà dans la prochaine section.

# 9. Exemple: HGLM avec les données ADORE

Nous sommes maintenant prêt.e.s à appliquer notre modèle dans R.

Pour mieux visualiser les relations le long de la distribution des points, nous ferons cette fois-ci un HLM en utilisant le log de la concentration effective moyenne. Notre modèle devra donc respecter les suppositions de base des modèles linéaires.

Rappelons le modèle:

$$\eta = X\beta + Z\gamma + U_1\delta_1 + U_2\delta_2 + 1\beta_0 + \epsilon$$

où les ordonnées à l'origine et les pentes aléatoires suivent une distribution Gaussienne multivariée tel que:

$$
\begin{bmatrix} \delta_1 \\ \delta_2 \end{bmatrix} \sim \text{MVN} \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \Omega \right)
$$

où $\Omega$ est une matrice de covariance tel que:

$$
\Omega = \begin{bmatrix} \text{var}(\delta_1) & \text{cov}(\delta_1, \delta_2) \\ \text{cov}(\delta_2, \delta_1) & \text{var}(\delta_2) \end{bmatrix}
$$

Nous verrons plus tard comment cette matrice de covariance nous permet de savoir si la concentration effective moyenne des espèces est associée à leur réponse à la température du médium.

## Construction du modèle

La librairie `brms` utilise les conventions de `lme4` pour spécifier les modèles.

Ainsi, on indique qu'on veut que le modèle estime:

- des ordonnées à l'origine aléatoires pour chaque espèce `(1 | tax_gs)`
- des pentes aléatoires pour chaque espèce `(1 + Zmedia_temperature_mean | tax_gs)`

En indiquant le terme `(1 + Zmedia_temperature_mean | tax_gs)`, le modèle effectuera les deux estimations.

```{r}
form2 <- brmsformula(
  result_conc1_mean_log ~
  test_exposure_type +
  Ztax_lh_amd +
  Zmedia_temperature_mean +
  (1 + Zmedia_temperature_mean | tax_gs)
)
```

## Informations à priori

On doit à nouveau spécifier les informations à priori. Ici, on utilise les mêmes informations que pour le glm, mais **on ajoute**:  

- les informations à priori sur les **deux paramètres de variance**
- les informations à priori sur la **matrice de covariance**

On spécifie que les écarts-type des ordonnées à l'origine et des pentes aléatoires suivront une distribution Gaussienne tronquée. `brms` spécifie automatiquement une distribution tronquée lorsqu'on l'applique à un paramètre de variance car elle ne peut aller sous 0.

Pour la matrice de covariance, le prior `lkj(2)` spécifie qu'on pénalise des covariances qui sont trop élevées. Ainsi, notre modèle sera plus conservateur.

```{r}
priors2 <- c(
  # Informations à priori sur l'ordonnée à l'origine
  set_prior(
    "normal(0, 1)",
    class = "Intercept"
  ),
  # Informations à priori sur les différences de moyenne
  set_prior(
    "normal(0, 1)",
    class = "b",
    coef = c(
      "test_exposure_typeNR",
      "test_exposure_typeR",
      "test_exposure_typeS"
    )
  ),
  # Informations à priori sur les pentes
  set_prior(
    "normal(0, 1)",
    class = "b",
    coef = c(
      "Zmedia_temperature_mean",
      "Ztax_lh_amd"
    )
  ),
  # Informations à priori sur les ordonnées à l'origine aléatoires
  set_prior(
    "normal(0, 1)",
    class = "sd",
    coef = "Intercept",
    group = "tax_gs"
  ),
  # Informations à priori sur les pentes aléatoires
  set_prior(
    "normal(0, 1)",
    class = "sd",
    coef = "Zmedia_temperature_mean",
    group = "tax_gs"
  ),
  # Informations à priori sur la matrice de covariance
  set_prior(
    "lkj(2)",
    class = "L",
    group = "tax_gs"
  )
)
```

## Estimation des paramètres

### Spécifications pour l'estimation MCMC

Voici les spécifications pour l'estimation des paramètres par HMC-NUTS. Pour chacune des 4 chaînes, nous aurons:

- n: 5000
- réchauffement: n - 1000
- itérations: n - réchauffement
- échantillonnage: itérations / échantillonnage = 1000

Notre modèle fera donc 1000 réchauffements (`warmup = 1000`), 4000 itérations (`iter = 5000` où 5000-1000), et échantillonera une valeur de paramètre par bonds de 4 itérations (`thin = 4`).

Nous obtiendrons ainsi 1000 échantillons par chaîne que nous combinerons pour un total de **4000 échantillons postérieurs par paramètre**.

J'ai augmenté le nombre d'itérations pour améliorer la convergence des chaînes.

### Exécution de l'estimateur

```{r hglm, cache=TRUE}
hglm <- brm(

  # La formule spécifiée
  formula = form2,

  # La famille de distribution
  family = gaussian(),

  # Les paramètres d'itérations pour le MCMC
  iter = 5000,
  warmup = 1000,
  thin = 4,
  chains = 4,

  # Paramètres pour accélérer les calculs
  # Utiliser seulement si vous avez >4 coeurs
  threads = threading(3),

  # Si vous avez 4 coeurs ou moins
  # cores = parallel::detectCores()-1,

  # Estimateur
  backend = "cmdstanr",

  # Reproductibilité
  seed = 123,

  # Paramètres de contrôle
  control = list(adapt_delta = 0.95),

  # Distributions à priori
  prior = priors2,

  # Échantillonage des distributions à priori
  # Ceci peut être utile à des fins de comparaison
  sample_prior = TRUE,

  # Données
  data = anim
)
```

On sauvegarde le modèle en format `.rds` qui pourra facilement être réimporté dans la session au besoin.

```{r}
saveRDS(
  object = hglm,
  file = file.path(chemin, "hglm.rds")
)
```

## Vérifications du modèle

On extrait l'ensemble des distributions postérieures.

```{r}
params2 <- as_draws_df(
  x = hglm, add_chain = TRUE,
  regex = TRUE
)
```

### Convergence des chaînes

Ici, nous devrions vérifier la convergence des chaînes pour tous les paramètres, mais nous allons nous limiter aux effets fixes, aux paramètres de variance, et à la matrice de covariance.

```{r plot18, cache=TRUE}
pars2 <- c(
  "b_Intercept", "b_test_exposure_typeNR", "b_test_exposure_typeR",
  "b_test_exposure_typeS", "b_Ztax_lh_amd", "b_Zmedia_temperature_mean",
  "sd_tax_gs__Intercept", "sd_tax_gs__Zmedia_temperature_mean",
  "cor_tax_gs__Intercept__Zmedia_temperature_mean"
)

bayesplot::mcmc_trace(
  params2,
  pars = pars2
)
```

Les chaînes de notre modèle semblent avoir très bien convergé.

Vérifions maintenant le $\hat{R}$, le `Bulk_ESS`, et le `Tail_ESS` pour **les effets fixes**:

```{r}
hglm_t1 <- data.table(
  round(summary(hglm, prob = 0.89, robust = TRUE)$fixed, digits = 3),
  keep.rownames = TRUE
)
setnames(hglm_t1, old = "rn", new = "Parameter")

hglm_t1[, c(1, 6:8)]
```

Les valeurs sont très bien!

Maintenant, la même procédure pour **les effets aléatoires**:

```{r}
hglm_t2 <- data.table(
  round(summary(hglm, prob = 0.89, robust = TRUE)$random$tax_gs, digits = 3),
  keep.rownames = TRUE
)
setnames(hglm_t2, old = "rn", new = "Parameter")

hglm_t2[, c(1, 6:8)]
```

Tout est beau!

### Ajustement du modèle

Examinons l'ajustement avec les vérifications prédictives postérieures.

```{r plot19, fig.height=5, fig.width=6, cache=TRUE}
pp_check(hglm, ndraws = 100)
```

Le modèle ne semble pas aussi bien ajusté aux données que celui d'avant. Toutefois, pour continuer l'atelier, nous le garderons comme ça. Il serait potentiellement possible de l'améliorer en utilisant des informations à priori plus informatives par exemple.

Finalement, on doit s'assurer que notre modèle respecte les conditions des modèles linéaires par rapport aux résidus.

```{r, cache=TRUE}
fitt <- fitted(hglm)[, 1]
res <- residuals(hglm)[, 1]
```

```{r plot20, fig.height=5, fig.width=6, cache=TRUE}
inspect_hglm <- ggplot() +
  geom_point(aes(x = fitt, y = res), alpha = 0.05) +
  geom_hline(yintercept = 0, colour = "red", linetype = "dashed") +
  scale_y_continuous(breaks = seq(-8, 8, 4), limits = c(-8, 8)) +
  scale_x_continuous(breaks = seq(-4, 2, 2), limits = c(-4, 2.5)) +
  xlab("Valeurs prédites") + ylab("Résidus normalisés") +
  theme_bw(base_size = 14) +
  theme(panel.grid = element_blank())

inspect_hglm <- ggExtra::ggMarginal(
  inspect_hglm,
  type = "histogram",
  margins = "y"
)

inspect_hglm
```

Les résidus sont suffisemment homogènes autour des valeurs prédites, et ils suivent une distribution Gaussienne!

En somme, nos diagnostics indiquent que notre modèle semble bon!

## Résultats

### Sommaire des paramètres

#### Effets fixes

Commençons par vérifier les paramètres des **effets fixes**:

```{r}
knitr::kable(hglm_t1[, c(1, 2, 4, 5)])
```

#### Effets aléatoires

Ensuite, les paramètres des **effets aléatoires**:

```{r}
knitr::kable(hglm_t2[, c(1, 2, 4, 5)])
```

Il semble y avoir une importante variation dans les moyennes entre les espèces ainsi que leur réaction à la température.

La corrélation positive entre les ordonnées à l'origine et les pentes indique que plus une espèce à une sensibilité élevée, plus elle est susceptible de répondre fortement à la température.

### Figures

#### Prédictions des effets fixes

**Température**

```{r}
tab3 <- conditional_effects(
  x = hglm, method = "posterior_epred",
  effects = "Zmedia_temperature_mean",
  spaghetti = TRUE, ndraws = 100,
  robust = TRUE, prob = 0.89
)
spag3 <- attr(tab3$Zmedia_temperature_mean, "spaghetti")
tab3 <- data.table(tab3[[1]])
```

**Longévité**

```{r}
tab4 <- conditional_effects(
  x = hglm, method = "posterior_epred",
  effects = "Ztax_lh_amd",
  spaghetti = TRUE, ndraws = 100,
  robust = TRUE, prob = 0.89
)
spag4 <- attr(tab4$Ztax_lh_amd, "spaghetti")
tab4 <- data.table(tab4[[1]])
```

**Type d'exposition**

On va utiliser les distributions postérieures des moyennes prédites de chaque type d'exposition comme on l'a fait avec le GLM. Nous avons déjà extrait ceci durant les inspections et assigné à l'objet `params2`.

Nous allons utiliser les 4 premières colones pour produire le graphique. Ces 4 colones sont:

- `b_Intercept`: la moyenne de l'exposition de type `F`
- `b_test_exposure_typeNR` est la différence entre `NR` et `F`
- `b_test_exposure_typeR` est la différentre entre `R` et `F`
- `b_test_exposure_typeS` est la différentre entre `S` et `F`

On veut:  

1. calculer la moyenne pour les expositions `NR`, `R`, et `S`
2. calculer la médiane des distributions postérieures

Faisons **l'étape 1**:

```{r}
# Transformer en data.table pour manipuler
paramsdt2 <- data.table(params2)

# On calcule les vraies moyennes
paramsdt2[, c(2:4) := paramsdt2[, c(2:4)] + b_Intercept]

# Changer en format long
preds2 <- melt(
  data = paramsdt2[, c(1:4)],
  variable.name = "exposure_type",
  measure = colnames(paramsdt2[, c(1:4)])
)
```

Et **l'étape 2**:

```{r}
# Calculer les médianes
medians2 <- preds2[, .(median = median(value)), by = exposure_type]
```

#### Prédictions des effets aléatoires

Pour les effets aléatoires, on veut extraire les prédictions pour chaque espèce. Voici la procédure :

1. Extraire les prédictions par espèce
2. Calculer la température minimum et maximum de chaque espèce dans les données bruts
3. Couper les droites estimées pour qu'elles ne couvrent que le minimum et maximum par espèce

Commençons par **l'étape 1**:

```{r, cache=TRUE}
tab5 <- conditional_effects(
  hglm, method = "fitted",
  effects = "Zmedia_temperature_mean:tax_gs",
  robust = TRUE, re_formula = NULL
)
tab5 <- data.table(tab5[[1]])
```

Ensuite, **l'étape 2**:

```{r}
ranges <- anim[
  , .(
    min = min(Zmedia_temperature_mean, na.rm = TRUE),
    max = max(Zmedia_temperature_mean, na.rm = TRUE)
  ),
  by = tax_gs
]
```

Finalement, **l'étape 3**. Cette étape demande de combiner la table `ranges` qu'on a fait à l'étape 2 avec `tab5` qu'on a fait à l'étape 1. Ici, vous verrez qu'on retire toute espèce où il n'y avait qu'une seule valeur de température en faisant `tab5[min != max, ]`.

```{r}
# Combiner les tables
tab5 <- merge(tab5, ranges, by = "tax_gs")

# Retirer toute espèce qui n'avait qu'une observation de température
tab5 <- tab5[min != max, ]

# Couper pour avoir des prédictions qui suivent l'étendue originale
tab5 <- tab5[Zmedia_temperature_mean <= max, ]
tab5 <- tab5[Zmedia_temperature_mean >= min, ]

# Compter le nombre d'observations par espèce pour le graphique
tab5[, count_fitted := length(unique(estimate__)), by = tax_gs]
```

#### Visualisation

On peut maintenant produire les graphiques. À titre de rappel, on a la même pente pour l'ensemble des types d'exposition, et des ordonnées à l'origine qui diffèrent (c-à-d, les moyennes de concentration diffèrent). Ici, la droite correspond au type d'exposition `F`.

Commençons avec les différences de moyennes.

```{r plot21, fig.height=5, fig.width=6, cache=TRUE}
ggplot(
  data = preds2,
  aes(x = value, fill = exposure_type)
) +
  geom_density(alpha = 0.5, colour = "black") +
  geom_vline(
    data = medians2,
    aes(xintercept = median, color = exposure_type),
    linetype = "dashed"
  ) +
  scale_fill_manual(
    labels = c(
      "Flux", "NS",
      "Renouvellement", "Statique"
    ),
    values = c("#440154", "#3d4d8a", "#24878e", "#40bd72")
  ) +
  scale_colour_manual(
    labels = c(
      "Flux", "NS",
      "Renouvellement", "Statique"
    ),
    values = c("#440154", "#3d4d8a", "#24878e", "#40bd72")
  ) +
  #scale_x_continuous(breaks = seq(0.25, 1, 0.25), limits = c(0.25, 1)) +
  xlab("\nlog(concentration effective (mg/L))") +
  ylab("Densité\n") +
  labs(fill = "Exposition", colour = "Exposition") +
  theme_classic(base_size = 14) +
  theme(legend.position = "top")
```

Ensuite, les droites pour la température et la longévité pour le type d'exposition `F`.

```{r plot22, fig.height=4, fig.width=8, cache = TRUE}
hglm_p1 <- ggplot() +
  geom_point(
    data = anim,
    aes(
      y = result_conc1_mean_log,
      x = Zmedia_temperature_mean
    ),
    alpha = 0.05
  ) +
  geom_line(
    data = spag3,
    aes(
      x = Zmedia_temperature_mean,
      y = estimate__,
      group = sample__
    ),
    linewidth = 1, alpha = 0.05,
    colour = "#440154"
  ) +
  geom_line(
    data = tab3,
    aes(
      x = Zmedia_temperature_mean,
      y = estimate__
    ),
    linewidth = 0.5,
    colour = "yellow"
  ) +
  ylab("log(concentration effective (mg/L))\n") +
  scale_y_continuous(
    breaks = seq(-8, 8, 4),
    limits = c(-10, 8)
  ) +
  xlab("\nTempérature moyenne normalisée") +
  scale_x_continuous(
    breaks = seq(-4, 4, 2),
    limits = c(-4, 4)
  ) +
  theme_classic(base_size = 14)

hglm_p2 <- ggplot() +
  geom_point(
    data = anim[Ztax_lh_amd <= 6],
    aes(
      y = result_conc1_mean_log,
      x = Ztax_lh_amd
    ),
    alpha = 0.05
  ) +
  geom_line(
    data = spag4[spag4$Ztax_lh_amd <= 6, ],
    aes(
      x = Ztax_lh_amd,
      y = estimate__,
      group = sample__
    ),
    linewidth = 1, alpha = 0.05,
    colour = "#440154"
  ) +
  geom_line(
    data = tab4[Ztax_lh_amd <= 6],
    aes(
      x = Ztax_lh_amd,
      y = estimate__
    ),
    linewidth = 0.5,
    colour = "yellow"
  ) +
  ylab("log(concentration effective (mg/L))\n") +
  scale_y_continuous(
    breaks = seq(-8, 8, 4),
    limits = c(-10, 8)
  ) +
  xlab("\nLongévité moyenne normalisée") +
  scale_x_continuous(
    breaks = seq(-2, 6, 2),
    limits = c(-2, 6.1)
  ) +
  theme_classic(base_size = 14)

ggarrange(hglm_p1, hglm_p2)
```

Finalement, nous avons les droites pour chaque espèce. Ici, montre les droites pour les espèces qui ont au moins 10 observations de température.

```{r plot23, fig.height=5, fig.width=6, cache = TRUE}
ggplot(
  tab5[count_fitted >= 10],
  aes(
    x = Zmedia_temperature_mean,
    y = estimate__,
    color = tax_gs
  )
) +
  geom_line(linewidth = 1, alpha = 0.5) +
  scale_color_viridis(discrete = TRUE, option = "D") +
  ylab("log(concentration effective (mg/L))") +
  xlab("Température moyenne normalisée") +
  scale_x_continuous(breaks = seq(-4, 4, 2), limits = c(-4, 4)) +
  scale_y_continuous(breaks = seq(-4, 4, 2), limits = c(-4, 4)) +
  theme_classic(base_size = 14) +
  theme(legend.position = "none")
```

## Interprétations

Nos résultats montrent que:

Les résultats sont très différents du GLM

donner des pistes de réflexion...

parce qu'on pool les valeurs entre 0 et 1.
ici on prends les vraies moyennes de la concentration

# Informations complémentaires

## Inspections additionnelles

While the model verifications that we just did are probably among the most important ones, they cover only a small portion of the checks that you should perform to ensure that your model has been correctly specified. To go further, here is a list of useful links for additional checks: 

Les vérifications de modèle que nous avons réalisées durant l'atelier sont parmi les plus importantes, toutefois, elles ne couvrent qu'une petite partie des vérifications que vous devriez effectuer pour vous assurer que votre modèle a été correctement spécifié. Pour aller plus loin, voici une liste de liens utiles pour des vérifications supplémentaires :

- [Diagnostiques des alertes de convergence](https://mc-stan.org/misc/warnings.html)
- [Diagnostiques MCMC](https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html) avec la librairie `bayesplot`
- [Vérifications prédictives postérieures additionnelles](https://mc-stan.org/bayesplot/articles/graphical-ppcs.html) avec la librairie `bayesplot`
- [Inspections des résidus additionnelles](http://mjskay.github.io/tidybayes/articles/tidybayes-residuals.html) avec la librairie `tidybayes`. Par exemple, vous pourriez évaluer la relation entre vos résidus et vous effets fixes. Ceci sera important pour les interprétations biologiques.
- [Validation croisée leave-one-out](https://link.springer.com/article/10.1007/s11222-016-9696-4) pour la performance prédictive et la comparaison de modèles. Consultez [le FAQ de la librairie loo](https://avehtari.github.io/modelselection/CV-FAQ.html), [l'exemple des coquerelles](https://avehtari.github.io/modelselection/roaches.html), et [la vignette de la librairie loo](http://mc-stan.org/loo/articles/index.html)

## Un atelier sur les MHGLM

Voici un autre atelier que j'ai développé, semblable à celui-ci, où on construit un modèle hiérarchique multivarié. Tous les documents sont trouvables dans [ce dépôt GitHub](https://github.com/quantitative-ecologist/ComputationalEcology-SummerSchool)

# References

<div id="refs"></div>

# Reproductibilité

Voici les informations minimales de mon système pour reproduire les analyses avec les mêmes versions de R ainsi que les librairies.

```{r sessionInfo}
sessionInfo()
```